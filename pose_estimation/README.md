## Environment
The code is developed using python 3.8.5 on Ubuntu 18.04. NVIDIA GPUs are needed. 

## Quick start
### Installation
1. Clone this repo as ${POSE_ROOT}
  
   ``` 
   git clone https://github.com/xuxu50007/previous_projects.git
   cd ${POSE_ROOT}
   ```
   
2. Install dependencies:
   
   ```
   pip install --upgrade pip
   pip install -r requirements.txt
   ```
   
3. Install [COCOAPI]

   ```
   # COCOAPI=/path/to/clone/cocoapi
   git clone https://github.com/cocodataset/cocoapi.git $COCOAPI
   cd $COCOAPI/PythonAPI
   # Install into global site-packages
   make install
   ```
   
4. Install [CrowdPoseAPI](https://github.com/Jeff-sjtu/CrowdPose) exactly the same as COCOAPI.  
    
   ```
   # CrowdPoseAPI=/path/to/clone/crowdposeapi
   git clone https://github.com/Jeff-sjtu/CrowdPose.git
   cd $CrowdPoseAPI/crowdpose-api/PythonAPI
   sh install.sh
   ``` 

5. Build dcn model:
   ```
   cd ${POSE_ROOT}
   #export CUDA_HOME=/gpfs/tools/amr/utils/cuda/cuda-10.1
   python setup.py develop
   ```
6. Init output(training model output directory) and log(tensorboard log directory) directory:

   ```
   mkdir output 
   mkdir log
   mkdir data
   mkdir models
   ```

   Your directory tree should look like as follows:

   ```
   ${POSE_ROOT}/
   ├── data
   ├── experiments
   ├── lib
   ├── log
   ├── models
   ├── output
   ├── tools 
   ├── README.md
   ├── requirements.txt
   └── setup.py
   ```

7. Download pretrained models and well-trained models from ([OneDrive](https://mailustceducn-my.sharepoint.com/:f:/g/personal/aa397601_mail_ustc_edu_cn/EgN4JcOE_KNHqG7coNOT_bABZvMWpaJxpy1J-9y1gduGcQ?e=PeeM2K)) and make models directory look like this:
    ```
    ${POSE_ROOT}
    |-- models
    `-- |-- imagenet
        |   |-- hrnet_w32-36af842e.pth
        |   `-- hrnetv2_w48_imagenet_pretrained.pth
        |-- pose_coco
        |   |-- pose_hrnet_w32_reg_delaysep_bg01_stn_512_adam_lr1e-3_coco_x140.pth
        |   |-- pose_hrnet_w48_reg_delaysep_bg01_stn_640_adam_lr1e-3_coco_x140.pth
        |   `-- pose_higher_hrnet_w48_reg_delaysep_bg01_0025_stn_640_adam_lr1e-3_coco_x140.pth
        |-- pose_crowdpose
        |   |-- pose_hrnet_w32_reg_delaysep_bg01_stn_512_adam_lr1e-3_crowdpose_x300.pth
        |   |-- pose_hrnet_w48_reg_delaysep_bg01_stn_640_adam_lr1e-3_crowdpose_x300.pth
        |   `-- pose_higher_hrnet_w48_reg_delaysep_bg01_0025_stn_640_adam_lr1e-3_crowdpose_x300.pth
        `-- rescore
            |-- final_rescore_coco_kpt.pth
            `-- final_rescore_crowd_pose_kpt.pth
    ```
   
### Data preparation
**For COCO data**, please download from [COCO download](http://cocodataset.org/#download). 
Download and extract them under {POSE_ROOT}/data, and make them look like this:

    ${POSE_ROOT}
    |-- data
    `-- |-- coco
        `-- |-- annotations
            |   |-- person_keypoints_train2017.json
            |   `-- person_keypoints_val2017.json
            `-- images
                |-- train2017.zip
                `-- val2017.zip

**For CrowdPose data**, please download from [CrowdPose download](https://github.com/Jeff-sjtu/CrowdPose#dataset).
Download and extract them under {POSE_ROOT}/data, and make them look like this:

    ${POSE_ROOT}
    |-- data
    `-- |-- crowdpose
        `-- |-- json
            |   |-- crowdpose_train.json
            |   |-- crowdpose_val.json
            |   |-- crowdpose_trainval.json (generated by tools/crowdpose_concat_train_val.py)
            |   `-- crowdpose_test.json
            `-- images.zip

After downloading data, run `python tools/crowdpose_concat_train_val.py` under `${POSE_ROOT}` to create trainval set.

### Training and Testing

#### Testing on COCO val2017 dataset without multi-scale test using well-trained pose model
 
```
python tools/valid.py \
    --cfg experiments/coco/w32/w32_4x_reg_delaysep_bg01_stn_512_adam_lr1e-3_coco_x140.yaml \
    TEST.MODEL_FILE models/pose_coco/pose_hrnet_w32_reg_delaysep_bg01_stn_512_adam_lr1e-3_coco_x140.pth
```

#### Testing on COCO val2017 dataset with multi-scale test using well-trained pose model
 
```
python tools/valid.py \
    --cfg experiments/coco/w32/w32_4x_reg_delaysep_bg01_stn_512_adam_lr1e-3_coco_x140.yaml \
    TEST.MODEL_FILE models/pose_coco/pose_hrnet_w32_reg_delaysep_bg01_stn_512_adam_lr1e-3_coco_x140.pth \ 
    TEST.SCALE_FACTOR 0.5,1,2
```

#### Training on COCO train2017 dataset

```
python tools/train.py \
    --cfg experiments/coco/w32/w32_4x_reg_delaysep_bg01_stn_512_adam_lr1e-3_coco_x140.yaml \
```

#### Using inference demo

```
python scripts/inference_demo_yan.py --cfg experiments/inference_demo_coco.yaml \
    --videoFile scripts/multi_people.mp4 \
    --outputDir output \
    --visthre 0.1 \
    TEST.MODEL_FILE model/pose_coco/pose_hrnet_w32_reg_delaysep_bg01_stn_512_adam_lr1e-3_coco_x140.pth
```
Note:
1. I write the new scripts under the folder "scripts":
    - "inference_demo_yan.py": to inference the output
    - "draw_skeleton.py": to draw the skeleton for each frame 
    - "write_keypoint_json.py": to write the keypoints into a json file
2. I recommend the value of "visthre" from 0.05 to 0.3.
3. If you want to save the image and video, add "--save_im_vd 1" in the inference demo.
4. The above command will create a video under "output" directory and a lot of pose images under "output/pose" directory. 
